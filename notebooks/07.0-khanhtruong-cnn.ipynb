{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from types import SimpleNamespace\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "import lightning as L\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib_inline.backend_inline\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import torchvision\n",
    "\n",
    "%matplotlib inline\n",
    "from IPython.display import HTML, display\n",
    "from lightning.pytorch.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats(\"svg\", \"pdf\")  # For export\n",
    "matplotlib.rcParams[\"lines.linewidth\"] = 2.0\n",
    "sns.reset_orig()\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchmetrics\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import pytorch_lightning as pl\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torchmetrics\n",
    "import math\n",
    "# PyTorch\n",
    "# Torchvision"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import glob\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DasDataset(Dataset):\n",
    "    def __init__(self, file_dir='../data/interim/', transform=None, target_transform=None):\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "        # file paths\n",
    "        self.file_paths = glob.glob(file_dir + \"/**/*.hdf5\", recursive = True)\n",
    "        file_shapes = []\n",
    "        max_val = 0\n",
    "        min_val = 0\n",
    "        for i in self.file_paths:\n",
    "            df = pd.read_hdf(i)\n",
    "            file_shapes.append(df.shape)\n",
    "            min_i = df.min().min()\n",
    "            max_i = df.max().max()\n",
    "            if  min_i < min_val:\n",
    "                min_val = min_i\n",
    "            if max_i > max_val:\n",
    "                max_val = max_i\n",
    "\n",
    "        self.max_val = max_val\n",
    "        self.min_val = min_val\n",
    "\n",
    "        # keep only files that have correct size (63, 50)\n",
    "        correct_idx = [index for index, value in enumerate(file_shapes) if value == (63, 50)]\n",
    "        self.file_paths = [self.file_paths[index] for index in correct_idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        image = pd.read_hdf(file_path).values.astype(np.float32)\n",
    "        image = (image - self.min_val) / (self.max_val - self.min_val)\n",
    "        image = torch.tensor(image).unsqueeze(0)\n",
    "        label = file_path.split('/')[3]\n",
    "        if label == 'rock':\n",
    "            label = 1\n",
    "        elif label == 'basketball':\n",
    "            label = 0\n",
    "        label = torch.tensor(label)\n",
    "        # if self.transform:\n",
    "        #     image = self.transform(image)\n",
    "        # if self.target_transform:\n",
    "        #     label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = DasDataset(transform=None, target_transform=None)\n",
    "torch.manual_seed(2252)\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(data, [100, 50, 71])\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=4, shuffle=False)\n",
    "val_loader = DataLoader(val_set, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_set, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 50 71\n"
     ]
    }
   ],
   "source": [
    "# checking data\n",
    "print(len(train_set), len(val_set), len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    117\n",
       "0    104\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking data\n",
    "labels = [i[1].item() for i in data]\n",
    "pd.Series(labels).value_counts()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define dataset by Lighning Data Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning.pytorch as pl\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "# Note - you must have torchvision installed for this example\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision import transforms\n",
    "\n",
    "\n",
    "class DasDataModule(pl.LightningDataModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.data_dir = data_dir\n",
    "        # self.transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_set, batch_size=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(val_set, batch_size=4)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(test_set, batch_size=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from torch.nn import Module\n",
    "from torch.nn import Conv2d\n",
    "from torch.nn import Linear\n",
    "from torch.nn import MaxPool2d\n",
    "from torch.nn import ReLU\n",
    "from torch.nn import LogSoftmax\n",
    "from torch import flatten"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kptruong/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "GPU available: True (mps), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/pytorch_lightning/trainer/setup.py:201: UserWarning: MPS available but not used. Set `accelerator` and `devices` using `Trainer(accelerator='mps', devices=1)`.\n",
      "  rank_zero_warn(\n",
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "  warning_cache.warn(\n",
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:615: UserWarning: Checkpoint directory /Users/kptruong/NTNU/dasly/notebooks exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name     | Type              | Params\n",
      "-----------------------------------------------\n",
      "0 | resnet   | ResNet            | 58.1 M\n",
      "1 | loss_fn  | BCEWithLogitsLoss | 0     \n",
      "2 | accuracy | BinaryAccuracy    | 0     \n",
      "-----------------------------------------------\n",
      "58.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "58.1 M    Total params\n",
      "232.558   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 12.36it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:432: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 8 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:280: PossibleUserWarning: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 25/25 [00:07<00:00,  3.16it/s, v_num=53, val_loss=1.160, val_acc=0.600]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 25/25 [00:07<00:00,  3.16it/s, v_num=53, val_loss=1.160, val_acc=0.600]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# ResNet model\n",
    "class ResNetModel(pl.LightningModule):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super().__init__()\n",
    "        self.resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet152', pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        in_features = self.resnet.fc.in_features\n",
    "        self.resnet.fc = nn.Linear(in_features, num_classes)\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.accuracy = torchmetrics.Accuracy(task='binary')\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits.squeeze(), y.float())\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        loss = self.loss_fn(logits.squeeze(), y.float())\n",
    "        acc = self.accuracy(torch.sigmoid(logits).squeeze(), y.float())\n",
    "        self.log('val_loss', loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=1e-3)\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        return self(batch)\n",
    "\n",
    "# Set up data directories and instantiate DataModule and ResNetModel\n",
    "data_dir = \"path/to/your/data/folders\"\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,)),\n",
    "])\n",
    "resnet_model = ResNetModel()\n",
    "\n",
    "\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor='val_acc',\n",
    "    dirpath='./',\n",
    "    filename='models-{epoch:02d}-{val_acc:.2f}',\n",
    "    save_top_k=1,\n",
    "    mode='max'\n",
    ")\n",
    "\n",
    "# Train the model using Lightning Trainer\n",
    "trainer = pl.Trainer(max_epochs=100, callbacks=[checkpoint_callback], accelerator='cpu') # , gpus=1 if torch.cuda.is_available() else 0\n",
    "trainer.fit(model=resnet_model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/kptruong/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kptruong/miniconda3/envs/das-env/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet152_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet152_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in /Users/kptruong/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "BEST_MODEL_PATH = \"/Users/kptruong/NTNU/dasly/notebooks/models-epoch=77-val_acc=0.96.ckpt\" #checkpoint_callback.best_model_path\n",
    "pretrained_model = ResNetModel().load_from_checkpoint(BEST_MODEL_PATH)\n",
    "pretrained_model.eval()\n",
    "pretrained_model.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = CustomImageDataset()\n",
    "# train_set, val_set, test_set = torch.utils.data.random_split(data, [100, 50, 71])\n",
    "\n",
    "# train_loader = DataLoader(train_set, batch_size=1, shuffle=True)\n",
    "# val_loader = DataLoader(val_set, batch_size=1, shuffle=True)\n",
    "# test_loader = DataLoader(test_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loader = test_loader\n",
    "\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in loader:  # Assuming test_loader returns (input, label) pairs, and we only care about inputs here\n",
    "        # print(labels)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = inputs.to(device)  # Move inputs to the same device as the model\n",
    "        # inputs = torch.unsqueeze(inputs, 0)  # Add a batch dimension to the input data\n",
    "        outputs = pretrained_model(inputs)\n",
    "        predicted = torch.greater(outputs, 0.5)\n",
    "        predictions.extend(predicted.cpu().numpy())  # Append predictions to the list\n",
    "\n",
    "predictions = [i[0].astype(int) for i in predictions]\n",
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91.5%\n"
     ]
    }
   ],
   "source": [
    "true_labels = [i[1].item() for i in loader.dataset]\n",
    "acc = sum([i == j for i, j in zip(predictions, true_labels)]) / len(true_labels)\n",
    "print(f'{acc:.1%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
